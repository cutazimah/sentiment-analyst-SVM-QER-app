{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a2c895-6b70-44e8-9e8b-38c405a6e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import defaultdict\n",
    "from sklearn.base import TransformerMixin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56cac7-4cbd-427d-a5ca-0791f094c823",
   "metadata": {},
   "source": [
    "## Data  Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227d4b96-4402-4a5b-97e9-5379c099cdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>username</th>\n",
       "      <th>sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Aug 04 00:51:01 +0000 2024</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>https://x.com/qwenvy/status/1819898803652366643</td>\n",
       "      <td>qwenvy</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Aug 05 06:10:13 +0000 2024</td>\n",
       "      <td>Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4</td>\n",
       "      <td>https://x.com/turfaul/status/1820341521125875955</td>\n",
       "      <td>turfaul</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 02 13:23:29 +0000 2024</td>\n",
       "      <td>Terimakasih @GrabID https://t.co/t3xsXonkg4</td>\n",
       "      <td>https://x.com/Zanboy04/status/1819363393100566662</td>\n",
       "      <td>Zanboy04</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed Aug 07 01:29:41 +0000 2024</td>\n",
       "      <td>Pagi ini @GrabID kok gabisa klik motor atau mo...</td>\n",
       "      <td>https://x.com/sipietz/status/1820995699078783021</td>\n",
       "      <td>sipietz</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed Jul 31 15:20:36 +0000 2024</td>\n",
       "      <td>makasih banyak fiturnya by... @GrabID https://...</td>\n",
       "      <td>https://x.com/moment4lifers/status/18186680915...</td>\n",
       "      <td>moment4lifers</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Sun Aug 04 00:51:01 +0000 2024   \n",
       "1  Mon Aug 05 06:10:13 +0000 2024   \n",
       "2  Fri Aug 02 13:23:29 +0000 2024   \n",
       "3  Wed Aug 07 01:29:41 +0000 2024   \n",
       "4  Wed Jul 31 15:20:36 +0000 2024   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1       Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4   \n",
       "2        Terimakasih @GrabID https://t.co/t3xsXonkg4   \n",
       "3  Pagi ini @GrabID kok gabisa klik motor atau mo...   \n",
       "4  makasih banyak fiturnya by... @GrabID https://...   \n",
       "\n",
       "                                           tweet_url       username sentimen  \n",
       "0    https://x.com/qwenvy/status/1819898803652366643         qwenvy  negatif  \n",
       "1   https://x.com/turfaul/status/1820341521125875955        turfaul   netral  \n",
       "2  https://x.com/Zanboy04/status/1819363393100566662       Zanboy04  positif  \n",
       "3   https://x.com/sipietz/status/1820995699078783021        sipietz   netral  \n",
       "4  https://x.com/moment4lifers/status/18186680915...  moment4lifers  positif  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"Data SA.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8cac06e-77ed-4ac6-9cc4-c1d589fb4f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>username</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>cleaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Aug 04 00:51:01 +0000 2024</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>https://x.com/qwenvy/status/1819898803652366643</td>\n",
       "      <td>qwenvy</td>\n",
       "      <td>negatif</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Aug 05 06:10:13 +0000 2024</td>\n",
       "      <td>Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4</td>\n",
       "      <td>https://x.com/turfaul/status/1820341521125875955</td>\n",
       "      <td>turfaul</td>\n",
       "      <td>netral</td>\n",
       "      <td>Diskon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 02 13:23:29 +0000 2024</td>\n",
       "      <td>Terimakasih @GrabID https://t.co/t3xsXonkg4</td>\n",
       "      <td>https://x.com/Zanboy04/status/1819363393100566662</td>\n",
       "      <td>Zanboy04</td>\n",
       "      <td>positif</td>\n",
       "      <td>Terimakasih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed Aug 07 01:29:41 +0000 2024</td>\n",
       "      <td>Pagi ini @GrabID kok gabisa klik motor atau mo...</td>\n",
       "      <td>https://x.com/sipietz/status/1820995699078783021</td>\n",
       "      <td>sipietz</td>\n",
       "      <td>netral</td>\n",
       "      <td>Pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed Jul 31 15:20:36 +0000 2024</td>\n",
       "      <td>makasih banyak fiturnya by... @GrabID https://...</td>\n",
       "      <td>https://x.com/moment4lifers/status/18186680915...</td>\n",
       "      <td>moment4lifers</td>\n",
       "      <td>positif</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Sun Aug 04 00:51:01 +0000 2024   \n",
       "1  Mon Aug 05 06:10:13 +0000 2024   \n",
       "2  Fri Aug 02 13:23:29 +0000 2024   \n",
       "3  Wed Aug 07 01:29:41 +0000 2024   \n",
       "4  Wed Jul 31 15:20:36 +0000 2024   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1       Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4   \n",
       "2        Terimakasih @GrabID https://t.co/t3xsXonkg4   \n",
       "3  Pagi ini @GrabID kok gabisa klik motor atau mo...   \n",
       "4  makasih banyak fiturnya by... @GrabID https://...   \n",
       "\n",
       "                                           tweet_url       username sentimen  \\\n",
       "0    https://x.com/qwenvy/status/1819898803652366643         qwenvy  negatif   \n",
       "1   https://x.com/turfaul/status/1820341521125875955        turfaul   netral   \n",
       "2  https://x.com/Zanboy04/status/1819363393100566662       Zanboy04  positif   \n",
       "3   https://x.com/sipietz/status/1820995699078783021        sipietz   netral   \n",
       "4  https://x.com/moment4lifers/status/18186680915...  moment4lifers  positif   \n",
       "\n",
       "                                            cleaning  \n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...  \n",
       "1                                          Diskon     \n",
       "2                                      Terimakasih    \n",
       "3    Pagi ini  kok gabisa klik motor atau mobil ya?   \n",
       "4                    makasih banyak fiturnya by...    "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import emoji\n",
    "\n",
    "# Fungsi untuk menghapus URL\n",
    "def remove_URL(tweet):\n",
    "    if tweet is not None and isinstance(tweet, str):\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        return url.sub(r'', tweet)\n",
    "    else:\n",
    "        return tweet\n",
    "\n",
    "# Fungsi untuk menghapus HTML\n",
    "def remove_html(tweet):\n",
    "    if tweet is not None and isinstance(tweet, str):\n",
    "        html = re.compile(r'<.*?>')\n",
    "        return html.sub(r'', tweet)\n",
    "    else:\n",
    "        return tweet\n",
    "\n",
    "# Fungsi untuk menghapus angka\n",
    "def remove_numbers(tweet):\n",
    "    if tweet is not None and isinstance(tweet, str):\n",
    "        tweet = re.sub(r'\\d', '', tweet)  # Menghapus semua angka\n",
    "    return tweet\n",
    "\n",
    "def remove_username(text):\n",
    "    import re\n",
    "    return re.sub(r'@[^\\s]+', '', text)\n",
    "\n",
    "# Mapping emotikon ke teks dalam bentuk string literal (bukan pattern regex)\n",
    "emot_map = {\n",
    "    \":-)\": \"senyum\",\n",
    "    \":)\": \"senyum\",\n",
    "    \":-D\": \"tertawa\",\n",
    "    \":D\": \"tertawa\",\n",
    "    \":'\\)\": \"menangis bahagia\",\n",
    "    \":-(\": \"sedih\",\n",
    "    \":(\": \"sedih\",\n",
    "    \":'‑(\": \"menangis\",\n",
    "    \":'(\": \"menangis\",\n",
    "    \":‑O\": \"terkejut\",\n",
    "    \":O\": \"terkejut\",\n",
    "    \":|\": \"datar\",\n",
    "    \":-/\": \"bingung\",\n",
    "    \":/\": \"bingung\",\n",
    "    \":-*\": \"cium\",\n",
    "    \":*\": \"cium\",\n",
    "    \";-)\": \"kedipan mata\",\n",
    "    \";)\": \"kedipan mata\",\n",
    "    \">:(\": \"marah\",\n",
    "    \":P\": \"menjulurkan lidah\",\n",
    "    \":-P\": \"menjulurkan lidah\",\n",
    "    \":3\": \"imut\",\n",
    "    \"-_-\": \"kesal\",\n",
    "    \"o_O\": \"bingung\",\n",
    "    \"O_o\": \"bingung\",\n",
    "    \">.<\": \"frustrasi\",\n",
    "    \"<3\": \"cinta\",\n",
    "    \"</3\": \"patah hati\",\n",
    "}\n",
    "\n",
    "# Escape semua key agar bisa digunakan dalam regex dengan aman\n",
    "escaped_keys = list(map(re.escape, emot_map.keys()))\n",
    "emot_regex = re.compile(\"|\".join(escaped_keys))\n",
    "\n",
    "# Fungsi untuk konversi\n",
    "def convert_emoticon(text):\n",
    "    text = emot_regex.sub(lambda m: emot_map[m.group()], str(text))  # gunakan str() untuk menghindari error NoneType\n",
    "    text = emoji.demojize(text)\n",
    "    return text.replace(\":\", \"\")\n",
    "\n",
    "data['cleaning'] = data['full_text'].apply(lambda x: remove_URL(x))\n",
    "data['cleaning'] = data['cleaning'].apply(lambda x: remove_username(x))\n",
    "data['cleaning'] = data['cleaning'].apply(lambda x: remove_html(x))\n",
    "data['cleaning'] = data['cleaning'].apply(lambda x: remove_numbers(x))\n",
    "data['cleaning'] = data['cleaning'].apply(lambda x: convert_emoticon(x))\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165cdb89-9216-44b7-8c30-c28f74090050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in c:\\users\\cut azimah - iffah\\appdata\\roaming\\python\\python310\\site-packages (2.14.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: translate in c:\\users\\cut azimah - iffah\\appdata\\roaming\\python\\python310\\site-packages (3.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\cut azimah - iffah\\appdata\\roaming\\python\\python310\\site-packages (from translate) (8.1.7)\n",
      "Requirement already satisfied: lxml in c:\\users\\cut azimah - iffah\\appdata\\roaming\\python\\python310\\site-packages (from translate) (5.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\cut azimah - iffah\\appdata\\roaming\\python\\python310\\site-packages (from translate) (2.32.3)\n",
      "Requirement already satisfied: libretranslatepy==2.1.1 in c:\\users\\cut azimah - iffah\\appdata\\roaming\\python\\python310\\site-packages (from translate) (2.1.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\sentiment-env\\lib\\site-packages (from click->translate) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cut azimah - iffah\\appdata\\roaming\\python\\python310\\site-packages (from requests->translate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cut azimah - iffah\\appdata\\roaming\\python\\python310\\site-packages (from requests->translate) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cut azimah - iffah\\appdata\\roaming\\python\\python310\\site-packages (from requests->translate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cut azimah - iffah\\appdata\\roaming\\python\\python310\\site-packages (from requests->translate) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!pip install translate\n",
    "\n",
    "import emoji\n",
    "from translate import Translator\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911f59fe-7622-4166-b9b6-1d584e73e7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>username</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>cleaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Aug 04 00:51:01 +0000 2024</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>https://x.com/qwenvy/status/1819898803652366643</td>\n",
       "      <td>qwenvy</td>\n",
       "      <td>negatif</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Aug 05 06:10:13 +0000 2024</td>\n",
       "      <td>Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4</td>\n",
       "      <td>https://x.com/turfaul/status/1820341521125875955</td>\n",
       "      <td>turfaul</td>\n",
       "      <td>netral</td>\n",
       "      <td>Diskon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 02 13:23:29 +0000 2024</td>\n",
       "      <td>Terimakasih @GrabID https://t.co/t3xsXonkg4</td>\n",
       "      <td>https://x.com/Zanboy04/status/1819363393100566662</td>\n",
       "      <td>Zanboy04</td>\n",
       "      <td>positif</td>\n",
       "      <td>Terimakasih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed Aug 07 01:29:41 +0000 2024</td>\n",
       "      <td>Pagi ini @GrabID kok gabisa klik motor atau mo...</td>\n",
       "      <td>https://x.com/sipietz/status/1820995699078783021</td>\n",
       "      <td>sipietz</td>\n",
       "      <td>netral</td>\n",
       "      <td>Pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed Jul 31 15:20:36 +0000 2024</td>\n",
       "      <td>makasih banyak fiturnya by... @GrabID https://...</td>\n",
       "      <td>https://x.com/moment4lifers/status/18186680915...</td>\n",
       "      <td>moment4lifers</td>\n",
       "      <td>positif</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Sun Aug 04 00:51:01 +0000 2024   \n",
       "1  Mon Aug 05 06:10:13 +0000 2024   \n",
       "2  Fri Aug 02 13:23:29 +0000 2024   \n",
       "3  Wed Aug 07 01:29:41 +0000 2024   \n",
       "4  Wed Jul 31 15:20:36 +0000 2024   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1       Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4   \n",
       "2        Terimakasih @GrabID https://t.co/t3xsXonkg4   \n",
       "3  Pagi ini @GrabID kok gabisa klik motor atau mo...   \n",
       "4  makasih banyak fiturnya by... @GrabID https://...   \n",
       "\n",
       "                                           tweet_url       username sentimen  \\\n",
       "0    https://x.com/qwenvy/status/1819898803652366643         qwenvy  negatif   \n",
       "1   https://x.com/turfaul/status/1820341521125875955        turfaul   netral   \n",
       "2  https://x.com/Zanboy04/status/1819363393100566662       Zanboy04  positif   \n",
       "3   https://x.com/sipietz/status/1820995699078783021        sipietz   netral   \n",
       "4  https://x.com/moment4lifers/status/18186680915...  moment4lifers  positif   \n",
       "\n",
       "                                            cleaning  \n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...  \n",
       "1                                          Diskon     \n",
       "2                                      Terimakasih    \n",
       "3    Pagi ini  kok gabisa klik motor atau mobil ya?   \n",
       "4                    makasih banyak fiturnya by...    "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def emoji_to_alias(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "def translate_alias_to_indonesian(text):\n",
    "    translator = Translator(to_lang=\"id\")\n",
    "    words = re.split(r'(:[^:]+:)', text)\n",
    "    translated_words = []\n",
    "    for word in words:\n",
    "        if word.startswith(':') and word.endswith(':'):\n",
    "            try:\n",
    "                word_clean = word.strip(':').replace('_', ' ')\n",
    "                translated = translator.translate(word_clean)\n",
    "                translated_words.append(translated)\n",
    "            except:\n",
    "                translated_words.append(word_clean)\n",
    "        else:\n",
    "            translated_words.append(word)\n",
    "    return ' '.join(translated_words)\n",
    "\n",
    "\n",
    "def clean_and_translate(text):\n",
    "    alias_text = emoji_to_alias(text)\n",
    "    return translate_alias_to_indonesian(alias_text)\n",
    "\n",
    "data['cleaning'] = data['cleaning'].apply(lambda x:clean_and_translate(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d29fc5c5-8663-4ee2-bbb8-20fda2399234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>username</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>case_folding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Aug 04 00:51:01 +0000 2024</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>https://x.com/qwenvy/status/1819898803652366643</td>\n",
       "      <td>qwenvy</td>\n",
       "      <td>negatif</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>shock bgt pagi²...woy ini rumah gw di pantau m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Aug 05 06:10:13 +0000 2024</td>\n",
       "      <td>Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4</td>\n",
       "      <td>https://x.com/turfaul/status/1820341521125875955</td>\n",
       "      <td>turfaul</td>\n",
       "      <td>netral</td>\n",
       "      <td>Diskon</td>\n",
       "      <td>diskon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 02 13:23:29 +0000 2024</td>\n",
       "      <td>Terimakasih @GrabID https://t.co/t3xsXonkg4</td>\n",
       "      <td>https://x.com/Zanboy04/status/1819363393100566662</td>\n",
       "      <td>Zanboy04</td>\n",
       "      <td>positif</td>\n",
       "      <td>Terimakasih</td>\n",
       "      <td>terimakasih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed Aug 07 01:29:41 +0000 2024</td>\n",
       "      <td>Pagi ini @GrabID kok gabisa klik motor atau mo...</td>\n",
       "      <td>https://x.com/sipietz/status/1820995699078783021</td>\n",
       "      <td>sipietz</td>\n",
       "      <td>netral</td>\n",
       "      <td>Pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "      <td>pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed Jul 31 15:20:36 +0000 2024</td>\n",
       "      <td>makasih banyak fiturnya by... @GrabID https://...</td>\n",
       "      <td>https://x.com/moment4lifers/status/18186680915...</td>\n",
       "      <td>moment4lifers</td>\n",
       "      <td>positif</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Sun Aug 04 00:51:01 +0000 2024   \n",
       "1  Mon Aug 05 06:10:13 +0000 2024   \n",
       "2  Fri Aug 02 13:23:29 +0000 2024   \n",
       "3  Wed Aug 07 01:29:41 +0000 2024   \n",
       "4  Wed Jul 31 15:20:36 +0000 2024   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1       Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4   \n",
       "2        Terimakasih @GrabID https://t.co/t3xsXonkg4   \n",
       "3  Pagi ini @GrabID kok gabisa klik motor atau mo...   \n",
       "4  makasih banyak fiturnya by... @GrabID https://...   \n",
       "\n",
       "                                           tweet_url       username sentimen  \\\n",
       "0    https://x.com/qwenvy/status/1819898803652366643         qwenvy  negatif   \n",
       "1   https://x.com/turfaul/status/1820341521125875955        turfaul   netral   \n",
       "2  https://x.com/Zanboy04/status/1819363393100566662       Zanboy04  positif   \n",
       "3   https://x.com/sipietz/status/1820995699078783021        sipietz   netral   \n",
       "4  https://x.com/moment4lifers/status/18186680915...  moment4lifers  positif   \n",
       "\n",
       "                                            cleaning  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1                                          Diskon      \n",
       "2                                      Terimakasih     \n",
       "3    Pagi ini  kok gabisa klik motor atau mobil ya?    \n",
       "4                    makasih banyak fiturnya by...     \n",
       "\n",
       "                                        case_folding  \n",
       "0  shock bgt pagi²...woy ini rumah gw di pantau m...  \n",
       "1                                          diskon     \n",
       "2                                      terimakasih    \n",
       "3    pagi ini  kok gabisa klik motor atau mobil ya?   \n",
       "4                    makasih banyak fiturnya by...    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def case_folding(text):\n",
    "    if isinstance(text, str):\n",
    "        lowercase_text = text.lower()\n",
    "        return lowercase_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "data['case_folding'] = data['cleaning'].apply(case_folding)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6102bc-54b5-4dbe-a4e4-4d08bc23832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fungsi penggantian kata tidak baku\n",
    "def replace_taboo_words(text, kamus_tidak_baku):\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        replaced_words = []\n",
    "        kalimat_baku = []\n",
    "        kata_diganti = []\n",
    "        kata_tidak_baku_hash = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in kamus_tidak_baku:\n",
    "                baku_word = kamus_tidak_baku[word]\n",
    "                if isinstance(baku_word, str) and all(char.isalpha() for char in baku_word):\n",
    "                    replaced_words.append(baku_word)\n",
    "                    kalimat_baku.append(baku_word)\n",
    "                    kata_diganti.append(word)\n",
    "                    kata_tidak_baku_hash.append(hash(word))\n",
    "                # else:\n",
    "                #     replaced_words.append('<karakter eksplisit>')\n",
    "                #     kalimat_baku.append(word)\n",
    "            else:\n",
    "                replaced_words.append(word)\n",
    "        replaced_text = ' '.join(replaced_words)\n",
    "    else:\n",
    "        replaced_text = ''\n",
    "        kalimat_baku = []\n",
    "        kata_diganti = []\n",
    "        kata_tidak_baku_hash = []\n",
    "\n",
    "    return replaced_text, kalimat_baku, kata_diganti, kata_tidak_baku_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70a9dcec-2a19-44ad-9cc0-fba5d38eab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baca dataset\n",
    "data_nor =  pd.DataFrame(data[['created_at','username','sentimen','full_text','cleaning','case_folding']])\n",
    "# data.head(5)\n",
    "\n",
    "# Baca kamus kata tidak baku\n",
    "kamus_data = pd.read_excel('kamuskatabaku.xlsx')\n",
    "kamus_tidak_baku = dict(zip(kamus_data['tidak_baku'], kamus_data['kata_baku']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e91f406-45e9-4352-926c-223db143b8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>username</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>case_folding</th>\n",
       "      <th>normalisasi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Aug 04 00:51:01 +0000 2024</td>\n",
       "      <td>qwenvy</td>\n",
       "      <td>negatif</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>shock bgt pagi²...woy ini rumah gw di pantau m...</td>\n",
       "      <td>shock banget pagi²...woy ini rumah gue di pant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Aug 05 06:10:13 +0000 2024</td>\n",
       "      <td>turfaul</td>\n",
       "      <td>netral</td>\n",
       "      <td>Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4</td>\n",
       "      <td>Diskon</td>\n",
       "      <td>diskon</td>\n",
       "      <td>diskon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 02 13:23:29 +0000 2024</td>\n",
       "      <td>Zanboy04</td>\n",
       "      <td>positif</td>\n",
       "      <td>Terimakasih @GrabID https://t.co/t3xsXonkg4</td>\n",
       "      <td>Terimakasih</td>\n",
       "      <td>terimakasih</td>\n",
       "      <td>terimakasih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed Aug 07 01:29:41 +0000 2024</td>\n",
       "      <td>sipietz</td>\n",
       "      <td>netral</td>\n",
       "      <td>Pagi ini @GrabID kok gabisa klik motor atau mo...</td>\n",
       "      <td>Pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "      <td>pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "      <td>pagi ini kok klik motor atau mobil ya?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed Jul 31 15:20:36 +0000 2024</td>\n",
       "      <td>moment4lifers</td>\n",
       "      <td>positif</td>\n",
       "      <td>makasih banyak fiturnya by... @GrabID https://...</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "      <td>banyak fiturnya by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at       username sentimen  \\\n",
       "0  Sun Aug 04 00:51:01 +0000 2024         qwenvy  negatif   \n",
       "1  Mon Aug 05 06:10:13 +0000 2024        turfaul   netral   \n",
       "2  Fri Aug 02 13:23:29 +0000 2024       Zanboy04  positif   \n",
       "3  Wed Aug 07 01:29:41 +0000 2024        sipietz   netral   \n",
       "4  Wed Jul 31 15:20:36 +0000 2024  moment4lifers  positif   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1       Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4   \n",
       "2        Terimakasih @GrabID https://t.co/t3xsXonkg4   \n",
       "3  Pagi ini @GrabID kok gabisa klik motor atau mo...   \n",
       "4  makasih banyak fiturnya by... @GrabID https://...   \n",
       "\n",
       "                                            cleaning  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1                                          Diskon      \n",
       "2                                      Terimakasih     \n",
       "3    Pagi ini  kok gabisa klik motor atau mobil ya?    \n",
       "4                    makasih banyak fiturnya by...     \n",
       "\n",
       "                                        case_folding  \\\n",
       "0  shock bgt pagi²...woy ini rumah gw di pantau m...   \n",
       "1                                          diskon      \n",
       "2                                      terimakasih     \n",
       "3    pagi ini  kok gabisa klik motor atau mobil ya?    \n",
       "4                    makasih banyak fiturnya by...     \n",
       "\n",
       "                                         normalisasi  \n",
       "0  shock banget pagi²...woy ini rumah gue di pant...  \n",
       "1                                             diskon  \n",
       "2                                        terimakasih  \n",
       "3             pagi ini kok klik motor atau mobil ya?  \n",
       "4                              banyak fiturnya by...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Terapkan fungsi penggantian kata tidak baku\n",
    "data_nor['normalisasi'], data_nor['Kata_Baku'], data_nor['Kata_Tidak_Baku'], data_nor['Kata_Tidak_Baku_Hash'] = zip(*data_nor['case_folding'].apply(lambda x: replace_taboo_words(x, kamus_tidak_baku)))\n",
    "# data_nor.head(100)\n",
    "\n",
    "data =  pd.DataFrame(data_nor[['created_at','username','sentimen','full_text','cleaning','case_folding','normalisasi']])\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9919d3d7-e79b-4084-b61f-b1a3b990b023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>username</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>case_folding</th>\n",
       "      <th>normalisasi</th>\n",
       "      <th>tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Aug 04 00:51:01 +0000 2024</td>\n",
       "      <td>qwenvy</td>\n",
       "      <td>negatif</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>shock bgt pagi²...woy ini rumah gw di pantau m...</td>\n",
       "      <td>shock banget pagi²...woy ini rumah gue di pant...</td>\n",
       "      <td>[shock, banget, pagi²...woy, ini, rumah, gue, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Aug 05 06:10:13 +0000 2024</td>\n",
       "      <td>turfaul</td>\n",
       "      <td>netral</td>\n",
       "      <td>Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4</td>\n",
       "      <td>Diskon</td>\n",
       "      <td>diskon</td>\n",
       "      <td>diskon</td>\n",
       "      <td>[diskon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 02 13:23:29 +0000 2024</td>\n",
       "      <td>Zanboy04</td>\n",
       "      <td>positif</td>\n",
       "      <td>Terimakasih @GrabID https://t.co/t3xsXonkg4</td>\n",
       "      <td>Terimakasih</td>\n",
       "      <td>terimakasih</td>\n",
       "      <td>terimakasih</td>\n",
       "      <td>[terimakasih]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed Aug 07 01:29:41 +0000 2024</td>\n",
       "      <td>sipietz</td>\n",
       "      <td>netral</td>\n",
       "      <td>Pagi ini @GrabID kok gabisa klik motor atau mo...</td>\n",
       "      <td>Pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "      <td>pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "      <td>pagi ini kok klik motor atau mobil ya?</td>\n",
       "      <td>[pagi, ini, kok, klik, motor, atau, mobil, ya?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed Jul 31 15:20:36 +0000 2024</td>\n",
       "      <td>moment4lifers</td>\n",
       "      <td>positif</td>\n",
       "      <td>makasih banyak fiturnya by... @GrabID https://...</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "      <td>banyak fiturnya by...</td>\n",
       "      <td>[banyak, fiturnya, by...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at       username sentimen  \\\n",
       "0  Sun Aug 04 00:51:01 +0000 2024         qwenvy  negatif   \n",
       "1  Mon Aug 05 06:10:13 +0000 2024        turfaul   netral   \n",
       "2  Fri Aug 02 13:23:29 +0000 2024       Zanboy04  positif   \n",
       "3  Wed Aug 07 01:29:41 +0000 2024        sipietz   netral   \n",
       "4  Wed Jul 31 15:20:36 +0000 2024  moment4lifers  positif   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1       Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4   \n",
       "2        Terimakasih @GrabID https://t.co/t3xsXonkg4   \n",
       "3  Pagi ini @GrabID kok gabisa klik motor atau mo...   \n",
       "4  makasih banyak fiturnya by... @GrabID https://...   \n",
       "\n",
       "                                            cleaning  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1                                          Diskon      \n",
       "2                                      Terimakasih     \n",
       "3    Pagi ini  kok gabisa klik motor atau mobil ya?    \n",
       "4                    makasih banyak fiturnya by...     \n",
       "\n",
       "                                        case_folding  \\\n",
       "0  shock bgt pagi²...woy ini rumah gw di pantau m...   \n",
       "1                                          diskon      \n",
       "2                                      terimakasih     \n",
       "3    pagi ini  kok gabisa klik motor atau mobil ya?    \n",
       "4                    makasih banyak fiturnya by...     \n",
       "\n",
       "                                         normalisasi  \\\n",
       "0  shock banget pagi²...woy ini rumah gue di pant...   \n",
       "1                                             diskon   \n",
       "2                                        terimakasih   \n",
       "3             pagi ini kok klik motor atau mobil ya?   \n",
       "4                              banyak fiturnya by...   \n",
       "\n",
       "                                            tokenize  \n",
       "0  [shock, banget, pagi²...woy, ini, rumah, gue, ...  \n",
       "1                                           [diskon]  \n",
       "2                                      [terimakasih]  \n",
       "3    [pagi, ini, kok, klik, motor, atau, mobil, ya?]  \n",
       "4                          [banyak, fiturnya, by...]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "data['tokenize'] = data['normalisasi'].apply(tokenize)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b486ad48-800a-46be-a3cb-0c44c737bf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Cut Azimah -\n",
      "[nltk_data]     Iffah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62f101c8-7230-4f47-a53b-ecb19bd1ff00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>username</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>case_folding</th>\n",
       "      <th>normalisasi</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>stopword removal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Aug 04 00:51:01 +0000 2024</td>\n",
       "      <td>qwenvy</td>\n",
       "      <td>negatif</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>shock bgt pagi²...woy ini rumah gw di pantau m...</td>\n",
       "      <td>shock banget pagi²...woy ini rumah gue di pant...</td>\n",
       "      <td>[shock, banget, pagi²...woy, ini, rumah, gue, ...</td>\n",
       "      <td>[shock, banget, pagi²...woy, rumah, gue, panta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Aug 05 06:10:13 +0000 2024</td>\n",
       "      <td>turfaul</td>\n",
       "      <td>netral</td>\n",
       "      <td>Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4</td>\n",
       "      <td>Diskon</td>\n",
       "      <td>diskon</td>\n",
       "      <td>diskon</td>\n",
       "      <td>[diskon]</td>\n",
       "      <td>[diskon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 02 13:23:29 +0000 2024</td>\n",
       "      <td>Zanboy04</td>\n",
       "      <td>positif</td>\n",
       "      <td>Terimakasih @GrabID https://t.co/t3xsXonkg4</td>\n",
       "      <td>Terimakasih</td>\n",
       "      <td>terimakasih</td>\n",
       "      <td>terimakasih</td>\n",
       "      <td>[terimakasih]</td>\n",
       "      <td>[terimakasih]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed Aug 07 01:29:41 +0000 2024</td>\n",
       "      <td>sipietz</td>\n",
       "      <td>netral</td>\n",
       "      <td>Pagi ini @GrabID kok gabisa klik motor atau mo...</td>\n",
       "      <td>Pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "      <td>pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "      <td>pagi ini kok klik motor atau mobil ya?</td>\n",
       "      <td>[pagi, ini, kok, klik, motor, atau, mobil, ya?]</td>\n",
       "      <td>[pagi, klik, motor, mobil, ya?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed Jul 31 15:20:36 +0000 2024</td>\n",
       "      <td>moment4lifers</td>\n",
       "      <td>positif</td>\n",
       "      <td>makasih banyak fiturnya by... @GrabID https://...</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "      <td>banyak fiturnya by...</td>\n",
       "      <td>[banyak, fiturnya, by...]</td>\n",
       "      <td>[fiturnya, by...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at       username sentimen  \\\n",
       "0  Sun Aug 04 00:51:01 +0000 2024         qwenvy  negatif   \n",
       "1  Mon Aug 05 06:10:13 +0000 2024        turfaul   netral   \n",
       "2  Fri Aug 02 13:23:29 +0000 2024       Zanboy04  positif   \n",
       "3  Wed Aug 07 01:29:41 +0000 2024        sipietz   netral   \n",
       "4  Wed Jul 31 15:20:36 +0000 2024  moment4lifers  positif   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1       Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4   \n",
       "2        Terimakasih @GrabID https://t.co/t3xsXonkg4   \n",
       "3  Pagi ini @GrabID kok gabisa klik motor atau mo...   \n",
       "4  makasih banyak fiturnya by... @GrabID https://...   \n",
       "\n",
       "                                            cleaning  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1                                          Diskon      \n",
       "2                                      Terimakasih     \n",
       "3    Pagi ini  kok gabisa klik motor atau mobil ya?    \n",
       "4                    makasih banyak fiturnya by...     \n",
       "\n",
       "                                        case_folding  \\\n",
       "0  shock bgt pagi²...woy ini rumah gw di pantau m...   \n",
       "1                                          diskon      \n",
       "2                                      terimakasih     \n",
       "3    pagi ini  kok gabisa klik motor atau mobil ya?    \n",
       "4                    makasih banyak fiturnya by...     \n",
       "\n",
       "                                         normalisasi  \\\n",
       "0  shock banget pagi²...woy ini rumah gue di pant...   \n",
       "1                                             diskon   \n",
       "2                                        terimakasih   \n",
       "3             pagi ini kok klik motor atau mobil ya?   \n",
       "4                              banyak fiturnya by...   \n",
       "\n",
       "                                            tokenize  \\\n",
       "0  [shock, banget, pagi²...woy, ini, rumah, gue, ...   \n",
       "1                                           [diskon]   \n",
       "2                                      [terimakasih]   \n",
       "3    [pagi, ini, kok, klik, motor, atau, mobil, ya?]   \n",
       "4                          [banyak, fiturnya, by...]   \n",
       "\n",
       "                                    stopword removal  \n",
       "0  [shock, banget, pagi²...woy, rumah, gue, panta...  \n",
       "1                                           [diskon]  \n",
       "2                                      [terimakasih]  \n",
       "3                    [pagi, klik, motor, mobil, ya?]  \n",
       "4                                  [fiturnya, by...]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in stop_words]\n",
    "data['stopword removal'] = data['tokenize'].apply(remove_stopwords)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a46d0b0-bcf9-4286-8194-19df31c2e0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Sastrawi in c:\\users\\cut azimah - iffah\\appdata\\roaming\\python\\python310\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install Sastrawi\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90647aed-522f-4e87-8b36-3b9501a92397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>username</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>case_folding</th>\n",
       "      <th>normalisasi</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>stopword removal</th>\n",
       "      <th>steming_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun Aug 04 00:51:01 +0000 2024</td>\n",
       "      <td>qwenvy</td>\n",
       "      <td>negatif</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...</td>\n",
       "      <td>shock bgt pagi²...woy ini rumah gw di pantau m...</td>\n",
       "      <td>shock banget pagi²...woy ini rumah gue di pant...</td>\n",
       "      <td>[shock, banget, pagi²...woy, ini, rumah, gue, ...</td>\n",
       "      <td>[shock, banget, pagi²...woy, rumah, gue, panta...</td>\n",
       "      <td>shock banget pagi woy rumah gue pantau maling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Aug 05 06:10:13 +0000 2024</td>\n",
       "      <td>turfaul</td>\n",
       "      <td>netral</td>\n",
       "      <td>Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4</td>\n",
       "      <td>Diskon</td>\n",
       "      <td>diskon</td>\n",
       "      <td>diskon</td>\n",
       "      <td>[diskon]</td>\n",
       "      <td>[diskon]</td>\n",
       "      <td>diskon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Aug 02 13:23:29 +0000 2024</td>\n",
       "      <td>Zanboy04</td>\n",
       "      <td>positif</td>\n",
       "      <td>Terimakasih @GrabID https://t.co/t3xsXonkg4</td>\n",
       "      <td>Terimakasih</td>\n",
       "      <td>terimakasih</td>\n",
       "      <td>terimakasih</td>\n",
       "      <td>[terimakasih]</td>\n",
       "      <td>[terimakasih]</td>\n",
       "      <td>terimakasih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed Aug 07 01:29:41 +0000 2024</td>\n",
       "      <td>sipietz</td>\n",
       "      <td>netral</td>\n",
       "      <td>Pagi ini @GrabID kok gabisa klik motor atau mo...</td>\n",
       "      <td>Pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "      <td>pagi ini  kok gabisa klik motor atau mobil ya?</td>\n",
       "      <td>pagi ini kok klik motor atau mobil ya?</td>\n",
       "      <td>[pagi, ini, kok, klik, motor, atau, mobil, ya?]</td>\n",
       "      <td>[pagi, klik, motor, mobil, ya?]</td>\n",
       "      <td>pagi klik motor mobil ya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed Jul 31 15:20:36 +0000 2024</td>\n",
       "      <td>moment4lifers</td>\n",
       "      <td>positif</td>\n",
       "      <td>makasih banyak fiturnya by... @GrabID https://...</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "      <td>makasih banyak fiturnya by...</td>\n",
       "      <td>banyak fiturnya by...</td>\n",
       "      <td>[banyak, fiturnya, by...]</td>\n",
       "      <td>[fiturnya, by...]</td>\n",
       "      <td>fiturnya by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sat Aug 03 06:38:11 +0000 2024</td>\n",
       "      <td>gabrieldyas39</td>\n",
       "      <td>positif</td>\n",
       "      <td>Promo @GrabID nggk kaleng2 diskon 55%. 50k cum...</td>\n",
       "      <td>Promo  nggk kaleng diskon %. k cuman byr k.</td>\n",
       "      <td>promo  nggk kaleng diskon %. k cuman byr k.</td>\n",
       "      <td>promo tidak kaleng diskon %. ke cuman byr k.</td>\n",
       "      <td>[promo, tidak, kaleng, diskon, %., ke, cuman, ...</td>\n",
       "      <td>[promo, kaleng, diskon, %., cuman, byr, k.]</td>\n",
       "      <td>promo kaleng diskon  cuman byr k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wed Aug 07 11:42:08 +0000 2024</td>\n",
       "      <td>myvalentine61</td>\n",
       "      <td>netral</td>\n",
       "      <td>Yg dijemput cewe only kah? @GrabID https://t.c...</td>\n",
       "      <td>Yg dijemput cewe only kah?</td>\n",
       "      <td>yg dijemput cewe only kah?</td>\n",
       "      <td>yang dijemput cewek only kah?</td>\n",
       "      <td>[yang, dijemput, cewek, only, kah?]</td>\n",
       "      <td>[dijemput, cewek, only, kah?]</td>\n",
       "      <td>jemput cewek only kah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fri Aug 09 23:31:47 +0000 2024</td>\n",
       "      <td>MrBeemss</td>\n",
       "      <td>negatif</td>\n",
       "      <td>Emang boleh ya ngakses card tanpa pin gini? Ya...</td>\n",
       "      <td>Emang boleh ya ngakses card tanpa pin gini? Ya...</td>\n",
       "      <td>emang boleh ya ngakses card tanpa pin gini? ya...</td>\n",
       "      <td>memang boleh ya ngakses card tanpa pin gini? y...</td>\n",
       "      <td>[memang, boleh, ya, ngakses, card, tanpa, pin,...</td>\n",
       "      <td>[ya, ngakses, card, pin, gini?, ya, maksudnya,...</td>\n",
       "      <td>ya ngakses card pin gin ya maksud pakai metode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fri Aug 09 04:00:01 +0000 2024</td>\n",
       "      <td>Beemoa2</td>\n",
       "      <td>negatif</td>\n",
       "      <td>Grab km petanya aneh bgt sumpahhhh driver gw s...</td>\n",
       "      <td>Grab km petanya aneh bgt sumpahhhh driver gw s...</td>\n",
       "      <td>grab km petanya aneh bgt sumpahhhh driver gw s...</td>\n",
       "      <td>grab kamu petanya aneh banget sumpahhhh driver...</td>\n",
       "      <td>[grab, kamu, petanya, aneh, banget, sumpahhhh,...</td>\n",
       "      <td>[grab, petanya, aneh, banget, sumpahhhh, drive...</td>\n",
       "      <td>grab peta aneh banget sumpahhhh driver gue nya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thu Aug 08 03:22:06 +0000 2024</td>\n",
       "      <td>Ropian_Taloko</td>\n",
       "      <td>netral</td>\n",
       "      <td>dah 3 jam ini blom bunyi bunyi min gimana ini ...</td>\n",
       "      <td>dah  jam ini blom bunyi bunyi min gimana ini  ...</td>\n",
       "      <td>dah  jam ini blom bunyi bunyi min gimana ini  ...</td>\n",
       "      <td>sudah jam ini belum bunyi bunyi min bagaimana ...</td>\n",
       "      <td>[sudah, jam, ini, belum, bunyi, bunyi, min, ba...</td>\n",
       "      <td>[jam, bunyi, bunyi, min, ordernya]</td>\n",
       "      <td>jam bunyi bunyi min order</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at       username sentimen  \\\n",
       "0  Sun Aug 04 00:51:01 +0000 2024         qwenvy  negatif   \n",
       "1  Mon Aug 05 06:10:13 +0000 2024        turfaul   netral   \n",
       "2  Fri Aug 02 13:23:29 +0000 2024       Zanboy04  positif   \n",
       "3  Wed Aug 07 01:29:41 +0000 2024        sipietz   netral   \n",
       "4  Wed Jul 31 15:20:36 +0000 2024  moment4lifers  positif   \n",
       "5  Sat Aug 03 06:38:11 +0000 2024  gabrieldyas39  positif   \n",
       "6  Wed Aug 07 11:42:08 +0000 2024  myvalentine61   netral   \n",
       "7  Fri Aug 09 23:31:47 +0000 2024       MrBeemss  negatif   \n",
       "8  Fri Aug 09 04:00:01 +0000 2024        Beemoa2  negatif   \n",
       "9  Thu Aug 08 03:22:06 +0000 2024  Ropian_Taloko   netral   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1       Diskon 6000 @GrabID? https://t.co/3aoxrdWnK4   \n",
       "2        Terimakasih @GrabID https://t.co/t3xsXonkg4   \n",
       "3  Pagi ini @GrabID kok gabisa klik motor atau mo...   \n",
       "4  makasih banyak fiturnya by... @GrabID https://...   \n",
       "5  Promo @GrabID nggk kaleng2 diskon 55%. 50k cum...   \n",
       "6  Yg dijemput cewe only kah? @GrabID https://t.c...   \n",
       "7  Emang boleh ya ngakses card tanpa pin gini? Ya...   \n",
       "8  Grab km petanya aneh bgt sumpahhhh driver gw s...   \n",
       "9  dah 3 jam ini blom bunyi bunyi min gimana ini ...   \n",
       "\n",
       "                                            cleaning  \\\n",
       "0  SHOCK BGT PAGI²...WOY INI RUMAH GW DI PANTAU M...   \n",
       "1                                          Diskon      \n",
       "2                                      Terimakasih     \n",
       "3    Pagi ini  kok gabisa klik motor atau mobil ya?    \n",
       "4                    makasih banyak fiturnya by...     \n",
       "5       Promo  nggk kaleng diskon %. k cuman byr k.    \n",
       "6                       Yg dijemput cewe only kah?     \n",
       "7  Emang boleh ya ngakses card tanpa pin gini? Ya...   \n",
       "8  Grab km petanya aneh bgt sumpahhhh driver gw s...   \n",
       "9  dah  jam ini blom bunyi bunyi min gimana ini  ...   \n",
       "\n",
       "                                        case_folding  \\\n",
       "0  shock bgt pagi²...woy ini rumah gw di pantau m...   \n",
       "1                                          diskon      \n",
       "2                                      terimakasih     \n",
       "3    pagi ini  kok gabisa klik motor atau mobil ya?    \n",
       "4                    makasih banyak fiturnya by...     \n",
       "5       promo  nggk kaleng diskon %. k cuman byr k.    \n",
       "6                       yg dijemput cewe only kah?     \n",
       "7  emang boleh ya ngakses card tanpa pin gini? ya...   \n",
       "8  grab km petanya aneh bgt sumpahhhh driver gw s...   \n",
       "9  dah  jam ini blom bunyi bunyi min gimana ini  ...   \n",
       "\n",
       "                                         normalisasi  \\\n",
       "0  shock banget pagi²...woy ini rumah gue di pant...   \n",
       "1                                             diskon   \n",
       "2                                        terimakasih   \n",
       "3             pagi ini kok klik motor atau mobil ya?   \n",
       "4                              banyak fiturnya by...   \n",
       "5       promo tidak kaleng diskon %. ke cuman byr k.   \n",
       "6                      yang dijemput cewek only kah?   \n",
       "7  memang boleh ya ngakses card tanpa pin gini? y...   \n",
       "8  grab kamu petanya aneh banget sumpahhhh driver...   \n",
       "9  sudah jam ini belum bunyi bunyi min bagaimana ...   \n",
       "\n",
       "                                            tokenize  \\\n",
       "0  [shock, banget, pagi²...woy, ini, rumah, gue, ...   \n",
       "1                                           [diskon]   \n",
       "2                                      [terimakasih]   \n",
       "3    [pagi, ini, kok, klik, motor, atau, mobil, ya?]   \n",
       "4                          [banyak, fiturnya, by...]   \n",
       "5  [promo, tidak, kaleng, diskon, %., ke, cuman, ...   \n",
       "6                [yang, dijemput, cewek, only, kah?]   \n",
       "7  [memang, boleh, ya, ngakses, card, tanpa, pin,...   \n",
       "8  [grab, kamu, petanya, aneh, banget, sumpahhhh,...   \n",
       "9  [sudah, jam, ini, belum, bunyi, bunyi, min, ba...   \n",
       "\n",
       "                                    stopword removal  \\\n",
       "0  [shock, banget, pagi²...woy, rumah, gue, panta...   \n",
       "1                                           [diskon]   \n",
       "2                                      [terimakasih]   \n",
       "3                    [pagi, klik, motor, mobil, ya?]   \n",
       "4                                  [fiturnya, by...]   \n",
       "5        [promo, kaleng, diskon, %., cuman, byr, k.]   \n",
       "6                      [dijemput, cewek, only, kah?]   \n",
       "7  [ya, ngakses, card, pin, gini?, ya, maksudnya,...   \n",
       "8  [grab, petanya, aneh, banget, sumpahhhh, drive...   \n",
       "9                 [jam, bunyi, bunyi, min, ordernya]   \n",
       "\n",
       "                                        steming_data  \n",
       "0  shock banget pagi woy rumah gue pantau maling ...  \n",
       "1                                             diskon  \n",
       "2                                        terimakasih  \n",
       "3                           pagi klik motor mobil ya  \n",
       "4                                        fiturnya by  \n",
       "5                   promo kaleng diskon  cuman byr k  \n",
       "6                              jemput cewek only kah  \n",
       "7  ya ngakses card pin gin ya maksud pakai metode...  \n",
       "8  grab peta aneh banget sumpahhhh driver gue nya...  \n",
       "9                          jam bunyi bunyi min order  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def stem_text(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "data['steming_data'] = data['stopword removal'].apply(lambda x: ' '.join(stem_text(x)))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18d3abfe-2fd5-4cbd-aeae-f3230d6e319f",
   "metadata": {},
   "outputs": [],
   "source": [
    " data.to_excel('dataset_15_06.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed61c90-4a8a-48eb-943a-18f3d4da637f",
   "metadata": {},
   "source": [
    "## Dropna data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b674c092-b6e1-40bf-a219-ebaeeb4de899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1140 entries, 0 to 1139\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   created_at        1140 non-null   object\n",
      " 1   username          1140 non-null   object\n",
      " 2   sentimen          1140 non-null   object\n",
      " 3   full_text         1140 non-null   object\n",
      " 4   cleaning          1140 non-null   object\n",
      " 5   case_folding      1140 non-null   object\n",
      " 6   normalisasi       1138 non-null   object\n",
      " 7   tokenize          1140 non-null   object\n",
      " 8   stopword removal  1140 non-null   object\n",
      " 9   steming_data      1134 non-null   object\n",
      "dtypes: object(10)\n",
      "memory usage: 89.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('dataset_15_06.xlsx')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38f2deab-ad45-4253-b1db-ffc695b86da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12c337e2-a22c-4e2c-b668-adc84f46d634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1134 entries, 0 to 1139\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   created_at        1134 non-null   object\n",
      " 1   username          1134 non-null   object\n",
      " 2   sentimen          1134 non-null   object\n",
      " 3   full_text         1134 non-null   object\n",
      " 4   cleaning          1134 non-null   object\n",
      " 5   case_folding      1134 non-null   object\n",
      " 6   normalisasi       1134 non-null   object\n",
      " 7   tokenize          1134 non-null   object\n",
      " 8   stopword removal  1134 non-null   object\n",
      " 9   steming_data      1134 non-null   object\n",
      "dtypes: object(10)\n",
      "memory usage: 97.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data = df()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6cba332-98aa-4394-944d-6aeea770e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    " data.to_excel('dataset_15_06_bersih.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896b954-58c7-45ef-86f7-cdc4a0196925",
   "metadata": {},
   "source": [
    "## label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c0add87-0b5c-4c11-b918-56f395ac280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label mapping: {'negatif': 0, 'netral': 1, 'positif': 2}\n",
      "label\n",
      "0    541\n",
      "1    460\n",
      "2    133\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_excel('dataset_15_06_bersih.xlsx')\n",
    "# Inisialisasi LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "data['label'] = encoder.fit_transform(data['sentimen'])\n",
    "\n",
    "#Melihat mapping\n",
    "label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(\"\\nLabel mapping:\", label_mapping)\n",
    "print(data['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c063415c-7330-489b-b240-206b88253a06",
   "metadata": {},
   "outputs": [],
   "source": [
    " data.to_excel('dataset_15_06_label_2.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219190f3-e79e-48bd-8e29-c96593a6ba1c",
   "metadata": {},
   "source": [
    "## UJICOBA  2 - FINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34719a78-9f52-4edc-8870-1e8437d15fae",
   "metadata": {},
   "source": [
    "🔧 1. Hitung QER untuk 3 kelas (rata-rata antar kelas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73181b52-0f59-4857-bbf9-5a84857f2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('dataset_15_06_label_2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbf0b48e-75fa-4cdf-804e-cedf9c0ade37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_qer_scores_multi_class_avg(data, label_col='label', token_col='steming_data'):\n",
    "    from collections import defaultdict\n",
    "    import math\n",
    "\n",
    "    doc_count = defaultdict(lambda: defaultdict(int))\n",
    "    class_counts = defaultdict(int)\n",
    "\n",
    "    for tokens, label in zip(data[token_col], data[label_col]):\n",
    "        class_counts[label] += 1\n",
    "        for token in set(tokens):  # LANGSUNG gunakan list token\n",
    "            doc_count[label][token] += 1\n",
    "\n",
    "    all_tokens = set()\n",
    "    for c in doc_count:\n",
    "        all_tokens.update(doc_count[c].keys())\n",
    "\n",
    "    classes = list(class_counts.keys())\n",
    "    qer_scores = {}\n",
    "\n",
    "    for token in all_tokens:\n",
    "        p = {}\n",
    "        for c in classes:\n",
    "            p[c] = (doc_count[c].get(token, 0) + 1) / (class_counts[c] + 1)\n",
    "\n",
    "        avg_scores = []\n",
    "        for i, c in enumerate(classes):\n",
    "            others = [oc for oc in classes if oc != c]\n",
    "            other_probs = sum([p[oc] for oc in others]) / len(others)\n",
    "            if other_probs == 0:\n",
    "                other_probs = 1e-6\n",
    "            score = abs(math.log(p[c] / other_probs))\n",
    "            avg_scores.append(score)\n",
    "\n",
    "        qer_scores[token] = sum(avg_scores) / len(avg_scores)\n",
    "\n",
    "    return qer_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca40896e-a79c-46be-b9c7-956de179fa53",
   "metadata": {},
   "source": [
    "🧪 3. Fungsi evaluasi semua eksperimen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a66bb-4f21-4681-b583-621d3ff1bfc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.sparse import diags\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "# # Ubah hanya jika masih string\n",
    "data['steming_data'] = data['steming_data'].apply(\n",
    "    lambda x: x.split() if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# # Pastikan 'steming_data' adalah list of token\n",
    "# data['filtered_text'] = data['steming_data'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "data['filtered_text'] = data['steming_data'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "def evaluate_experiment_3_safe_pipeline(data, top_k=150, alpha=0.5):\n",
    "    X_raw_all = data['filtered_text']         # hasil join dari list token\n",
    "    tokens_data_all = data['steming_data']    # list of tokens\n",
    "    y_all = data['label']\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    all_y_true, all_y_pred = [], []\n",
    "    top_tokens_per_fold = []\n",
    "    qer_scores_per_fold = []\n",
    "    fold_accuracies = []\n",
    "\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "    best_vectorizer = None\n",
    "    best_qer_scores = None\n",
    "\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_raw_all, y_all), 1):\n",
    "        print(f\"Fold {fold} dimulai...\")\n",
    "\n",
    "        print(f\"  Train size: {len(train_idx)}, Test size: {len(test_idx)}\")\n",
    "\n",
    "        # Split sesuai indeks\n",
    "        X_train_raw = X_raw_all.iloc[train_idx]\n",
    "        X_test_raw = X_raw_all.iloc[test_idx]\n",
    "        tokens_train = tokens_data_all.iloc[train_idx].tolist()\n",
    "        y_train = y_all.iloc[train_idx]\n",
    "        y_test = y_all.iloc[test_idx]\n",
    "\n",
    "        # Siapkan DataFrame untuk training subset\n",
    "        train_data = data.iloc[train_idx].copy()\n",
    "\n",
    "        # Hitung QER dari data training SAJA\n",
    "        qer_scores = compute_qer_scores_multi_class_avg(\n",
    "            train_data, label_col='label', token_col='steming_data'\n",
    "        )\n",
    "        top_tokens = sorted(qer_scores, key=qer_scores.get, reverse=True)[:top_k]\n",
    "\n",
    "        # Vectorizer dari data training SAJA\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train_vec = vectorizer.fit_transform(X_train_raw)\n",
    "        X_test_vec = vectorizer.transform(X_test_raw)\n",
    "\n",
    "        # --- Tampilkan Top 10 IDF Terms ---\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        idf_values = vectorizer.idf_\n",
    "        idf_df = pd.DataFrame(list(zip(feature_names, idf_values)), columns=['Word', 'IDF'])\n",
    "        top_10_idf = idf_df.sort_values(by='IDF', ascending=False).head(10)\n",
    "\n",
    "        # Hitung bobot QER berdasarkan vocab vectorizer\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        qer_weights = np.array([qer_scores.get(word, 1.0) for word in feature_names])\n",
    "        qer_weights = qer_weights ** alpha  # <<< Tuning di sini\n",
    "        W = diags(qer_weights)\n",
    "        X_train_vec = X_train_vec @ W\n",
    "        X_test_vec = X_test_vec @ W\n",
    "\n",
    "        # Oversampling dengan SMOTE\n",
    "        smote = SMOTE(random_state=42, sampling_strategy='not majority')\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "        # Latih model\n",
    "        model = SVC(C=1, kernel='linear', probability=True, random_state=42,  class_weight='balanced')\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Akurasi Fold {fold}: {acc:.4f}\")\n",
    "        fold_accuracies.append(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model = model\n",
    "            best_vectorizer = vectorizer\n",
    "            best_qer_scores = qer_scores\n",
    "\n",
    "        # Prediksi & simpan hasil\n",
    "        all_y_true.extend(y_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "\n",
    "        top_tokens_per_fold.append(top_tokens)\n",
    "        qer_scores_per_fold.append(qer_scores)\n",
    "\n",
    "        print(f\"Fold {fold} selesai.\")\n",
    "    \n",
    "     # === Simpan Model Terbaik ===\n",
    "    # print(\"\\nFold terbaik disimpan dengan akurasi: {:.4f}\".format(best_acc))\n",
    "    # joblib.dump(best_model, \"model_svm_qer_best.pkl\")\n",
    "    # joblib.dump(best_vectorizer, \"vectorizer_tfidf_best.pkl\")\n",
    "    # joblib.dump(best_qer_scores, \"qer_scores_best.pkl\")\n",
    "\n",
    "    # Evaluasi akhir\n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    print(classification_report(all_y_true, all_y_pred, target_names=['Kelas 0', 'Kelas 1', 'Kelas 2']))\n",
    "    print(\"=== Confusion Matrix ===\")\n",
    "    print(confusion_matrix(all_y_true, all_y_pred))\n",
    "\n",
    "     # Hitung Cohen's Kappa\n",
    "    kappa = cohen_kappa_score(all_y_true, all_y_pred)\n",
    "    print(f\"Cohen's Kappa Score: {kappa:.4f}\")\n",
    "    print(f\"Akurasi Rata-rata: {np.mean(np.array(all_y_true) == np.array(all_y_pred)):.4f}\")\n",
    "\n",
    "    #   # Hitung akurasi per fold\n",
    "    # acc = accuracy_score(y_test, y_pred)\n",
    "    # fold_accuracies.append(acc)\n",
    "    # print(f\"Akurasi Fold {fold}: {acc:.4f}\")\n",
    "\n",
    "        # Rata-rata akurasi semua fold\n",
    "    mean_acc = np.mean(fold_accuracies)\n",
    "    print(f\"\\nAkurasi Rata-rata dari Semua Fold: {mean_acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model = model\n",
    "        best_vectorizer = vectorizer\n",
    "        best_qer_scores = qer_scores\n",
    "\n",
    "\n",
    "     # Tambahkan return agar bisa diakses dari luar fungsi\n",
    "    # return all_y_true, all_y_pred, X_raw_all, top_tokens, qer_scores, top_tokens_per_fold, qer_scores_per_fold,  kappa\n",
    "    return all_y_true, all_y_pred, X_raw_all, top_tokens, qer_scores, top_tokens_per_fold, qer_scores_per_fold, kappa, best_model, best_vectorizer, mean_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f5db3e4-5525-4dc1-81d6-3ed85aeaf848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 dimulai...\n",
      "  Train size: 907, Test size: 227\n",
      "Akurasi Fold 1: 0.7489\n",
      "Fold 1 selesai.\n",
      "Fold 2 dimulai...\n",
      "  Train size: 907, Test size: 227\n",
      "Akurasi Fold 2: 0.7181\n",
      "Fold 2 selesai.\n",
      "Fold 3 dimulai...\n",
      "  Train size: 907, Test size: 227\n",
      "Akurasi Fold 3: 0.7093\n",
      "Fold 3 selesai.\n",
      "Fold 4 dimulai...\n",
      "  Train size: 907, Test size: 227\n",
      "Akurasi Fold 4: 0.6740\n",
      "Fold 4 selesai.\n",
      "Fold 5 dimulai...\n",
      "  Train size: 908, Test size: 226\n",
      "Akurasi Fold 5: 0.7522\n",
      "Fold 5 selesai.\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Kelas 0       0.74      0.79      0.77       541\n",
      "     Kelas 1       0.70      0.75      0.73       460\n",
      "     Kelas 2       0.66      0.31      0.42       133\n",
      "\n",
      "    accuracy                           0.72      1134\n",
      "   macro avg       0.70      0.62      0.64      1134\n",
      "weighted avg       0.72      0.72      0.71      1134\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[429 103   9]\n",
      " [101 347  12]\n",
      " [ 48  44  41]]\n",
      "Cohen's Kappa Score: 0.5128\n",
      "Akurasi Rata-rata: 0.7205\n",
      "\n",
      "Akurasi Rata-rata dari Semua Fold: 0.7205\n"
     ]
    }
   ],
   "source": [
    "# all_y_true, all_y_pred, top_tokens, qer_scores = evaluate_experiment_3_safe_pipeline(data)\n",
    "# all_y_true, all_y_pred, texts, top_tokens, qer_scores, top_tokens_per_fold, qer_scores_per_fold,  kappa = evaluate_experiment_3_safe_pipeline(data, alpha=0.5)\n",
    "(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    X_all,\n",
    "    last_top_tokens,\n",
    "    last_qer_scores,\n",
    "    top_tokens_all_folds,\n",
    "    qer_scores_all_folds,\n",
    "    kappa,\n",
    "    best_model,\n",
    "    best_vectorizer,\n",
    "    mean_accuracy\n",
    ") = evaluate_experiment_3_safe_pipeline(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b274d9a-ccc6-4a5f-8855-1d8940344552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
